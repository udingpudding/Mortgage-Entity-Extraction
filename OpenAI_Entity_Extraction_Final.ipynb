{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq41Soqbbm2k"
      },
      "source": [
        "#Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2jw_HYmvtE1",
        "outputId": "9c7f6db8-2df4-443b-ebd6-f872eaa80baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZuJ4hRQvdJY",
        "outputId": "03289239-a8b7-441b-91c5-c502b789fb36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu121\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "#utilities\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "#data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#deep learning\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "#llm\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT_FXx8IhRPm"
      },
      "source": [
        "# Utils/Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xcq7BjxwrQG"
      },
      "source": [
        "Input Format for All Functions - All functions expect the same input JSON\n",
        "\n",
        "- **First Level**: Keys represent document names, with values as dictionaries of entity extractions.\n",
        "- **Second Level**: Keys represent entity names, with values as the model's extraction.\n",
        "\n",
        "---\n",
        "\n",
        "Workflow Steps\n",
        "\n",
        "1. **Load OCR JSON**  \n",
        "   Load the nested JSON file containing entity extractions.\n",
        "\n",
        "2. **Check Keys**  \n",
        "   Ensure all columns (keys) are present across documents and have uniform naming conventions and expected JSON structures.\n",
        "\n",
        "3. **Check for Hallucination**  \n",
        "   Ensure the extractions are actually present in the OCR output.\n",
        "\n",
        "4. **Flatten**  \n",
        "   Transform the nested structure into a flattened format.\n",
        "\n",
        "5. **Convert to DataFrame**  \n",
        "   Convert the flattened data into a structured DataFrame for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr4i9Daf2nCY"
      },
      "outputs": [],
      "source": [
        "def load_ocr_json(path):\n",
        "\n",
        "  \"\"\"\n",
        "    Loads an OCR JSON file and extracts the document names and their corresponding texts.\n",
        "\n",
        "    Parameters:\n",
        "    - path (str): The path to the JSON file containing OCR data. The file should have a structure where\n",
        "                  keys represent document names and values are the extracted OCR text.\n",
        "\n",
        "    Returns:\n",
        "    - document_names (list): A list of document names (keys from the JSON).\n",
        "    - texts (list): A list of the OCR texts (values from the JSON).\n",
        "\n",
        "    Workflow:\n",
        "    1. Opens the OCR JSON file specified by the path.\n",
        "    2. Loads the contents of the file into a dictionary.\n",
        "    3. Iterates through the dictionary to extract the document names and corresponding texts.\n",
        "    4. Returns the document names and texts as two separate lists.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #Load OCR JSON\n",
        "  with open(path, 'r') as f:\n",
        "    loading = json.load(f)\n",
        "\n",
        "  #Storage\n",
        "  document_names = []\n",
        "  texts = []\n",
        "\n",
        "  #Get Document Names and OCRs\n",
        "  for k,v in loading.items():\n",
        "    document_names.append(k)\n",
        "    texts.append(v)\n",
        "\n",
        "  return document_names, texts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm-h5JYLhSz2"
      },
      "outputs": [],
      "source": [
        "#Twin Functions:\n",
        "\n",
        "    #1. First flattens the keys of a nested JSON. This just return the flattened key structure.\n",
        "    #2. Second calls the first to get flattened key names. It then checks if all records in the JSON have the same columns and structure.\n",
        "      #This function will be used to check the output of an LLM, to ensure that all requested entities are present, their nested entities are present, and their column names are uniform.\n",
        "\n",
        "def get_flattened_keys(json, parent_key=\"\"):\n",
        "\n",
        "    \"\"\"\n",
        "    Flattens the keys of a nested dictionary (input JSON) by concatenating parent and child keys with underscores.\n",
        "\n",
        "    Parameters:\n",
        "    - json (dict): The nested dictionary to process.\n",
        "    - parent_key (str, optional): Used internally to build the flattened keys for nested dictionaries. Defaults to an empty string.\n",
        "\n",
        "    Returns:\n",
        "    - keys (set): A set of flattened keys, where nested keys are concatenated with underscores.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "    keys = set()\n",
        "    for key, value in json.items():\n",
        "        full_key = f\"{parent_key}_{key}\" if parent_key else key\n",
        "        if isinstance(value, dict):\n",
        "            keys.update(get_flattened_keys(value, full_key))\n",
        "        else:\n",
        "            keys.add(full_key)\n",
        "    return keys\n",
        "\n",
        "\n",
        "def check_keys(json):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "    Checks whether all records in a dictionary have the same flattened key structure.\n",
        "\n",
        "    Parameters:\n",
        "    - json (dict): The model's output nested JSON, where each key represents a document\n",
        "                and maps to a nested dictionary with entities and their values.\n",
        "\n",
        "    Returns:\n",
        "    - None: Prints results to the console.\n",
        "\n",
        "    Workflow:\n",
        "    1. Extracts the nested dictionaries (each document's entity extraction) from the input dictionary.\n",
        "    2. Flattens the keys of the first record to serve as a baseline.\n",
        "    3. Compares the flattened keys of each subsequent record against the baseline.\n",
        "    4. Reports inconsistencies and highlights the expected vs. actual keys.\n",
        "\n",
        "    Example:\n",
        "    >>> data = {\n",
        "    ...     \"doc1\": {\"page1\": {\"text\": \"Hello\"}, \"page2\": {\"text\": \"World\"}},\n",
        "    ...     \"doc2\": {\"page1\": {\"text\": \"Hello\"}, \"page2\": {\"words\": 50}}\n",
        "    ... }\n",
        "    >>> check_keys(data)\n",
        "    Document 1 has different keys.\n",
        "    Expected keys: {'page1_text', 'page2_text'}\n",
        "    Found keys: {'page1_text', 'page2_words'}\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  #Extracting Independent Dictionaries\n",
        "  records = [] #list of dictionary, each dictionary is each document's extraction\n",
        "  for key, value in json.items():\n",
        "      record = value  # Get the nested dictionary\n",
        "      records.append(record)\n",
        "  json = records\n",
        "\n",
        "  #Flattening and Checking\n",
        "  if json:\n",
        "      # Get the flattened keys of the first document\n",
        "      base_keys = get_flattened_keys(json[0])\n",
        "\n",
        "      # Check each document for key consistency\n",
        "      all_keys_match = True\n",
        "      for i, entry in enumerate(json):\n",
        "          entry_keys = get_flattened_keys(entry)\n",
        "          if entry_keys != base_keys:\n",
        "              print(f\"Document {i} has different keys.\")\n",
        "              print(f\"Expected keys: {base_keys}\")\n",
        "              print(f\"Found keys: {entry_keys}\")\n",
        "              all_keys_match = False\n",
        "      #Printing Results\n",
        "      if all_keys_match:\n",
        "          print(\"All documents have the same keys.\")\n",
        "          print(\"Baseline keys:\", base_keys)\n",
        "      else:\n",
        "          print(\"Not all documents have the same keys.\")\n",
        "  else:\n",
        "      print(\"No documents found in the JSON file.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYfsbhSD19tn"
      },
      "outputs": [],
      "source": [
        "#These are two functions to handle the model's nested output JSON and prepare it to be converted into a DataFrame:\n",
        "  #1. The rec_flatten function will take an independent nested JSON structure and flatten it.\n",
        "  #2. The flatten_json function will take model's outputted JSON, call the rec_flatten function to get each independent JSON, and then output a list of flattened dictionaries.\n",
        "\n",
        "def rec_flatten(single_nested_json,parent_key='', sep='_'):\n",
        "\n",
        "  \"\"\"\n",
        "    Recursively flattens a nested dictionary, concatenating parent and child keys using a specified separator.\n",
        "\n",
        "    Parameters:\n",
        "    - json (dict): A single nested JSON to be flattened.\n",
        "    - parent_key (str, optional): The base key to prepend to the current key. Defaults to an empty string.\n",
        "    - sep (str, optional): The separator used to concatenate keys. Defaults to an underscore ('_').\n",
        "\n",
        "    Returns:\n",
        "    - dict: A flattened dictionary where nested keys are represented as concatenated strings.\n",
        "\n",
        "    Workflow:\n",
        "    1. Iterates through all key-value pairs in the dictionary.\n",
        "    2. Concatenates the parent key (if any) with the current key using the separator.\n",
        "    3. If the value is a dictionary, recursively flattens it and appends the results.\n",
        "    4. If the value is not a dictionary, adds the concatenated key and its value to the flattened dictionary.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  items = []\n",
        "  for key, value in single_nested_json.items():\n",
        "    new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
        "    if isinstance(value, dict):\n",
        "      items.extend(rec_flatten(value, new_key, sep=sep).items())\n",
        "    else:\n",
        "        items.append((new_key, value))\n",
        "\n",
        "  return dict(items)\n",
        "\n",
        "\n",
        "\n",
        "def flatten_json(json, parent_key='', sep='_'):\n",
        "    \"\"\"\n",
        "    Takes the model's outputted nested JSON, flattens each independent JSON by calling rec_flatten,\n",
        "    and outputs a list of flattened dictionaries - one for each document.\n",
        "\n",
        "    Args:\n",
        "    nested_json (dict): The model's outputted nested JSON.\n",
        "    parent_key (str): The base key to append to. Default is ''.\n",
        "    sep (str): The separator to use between parent and child keys. Default is '_'.\n",
        "\n",
        "    Returns:\n",
        "    list: list of flattened JSONs (dictionaries).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    #Storage\n",
        "    flattened_jsons= []\n",
        "\n",
        "    #Flattening and Appending\n",
        "    for key, value in json.items():\n",
        "      flattened_json = rec_flatten(value)\n",
        "      flattened_jsons.append(flattened_json)\n",
        "\n",
        "    return flattened_jsons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkjPYQns72vR"
      },
      "outputs": [],
      "source": [
        "#This function is a check for hallucination. It checks the model output with corresponding document's OCR\n",
        "# to check whether the extracted entities are actually present in the document.\n",
        "\n",
        "\n",
        "def check_hallucination(ocr_json, entity_json):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "    This function checks for hallucinations by verifying whether the entities extracted from the LLM\n",
        "    actually appear in the OCR output. It compares the document's extracted entities with the document's corresponding OCR text.\n",
        "\n",
        "    Parameters:\n",
        "    - ocr_json (dict): A dictionary containing the OCR outputs for each document.\n",
        "                        The keys represent document names and the values are the OCR texts.\n",
        "    - entity_json (dict): A nested dictionary representing the extracted entities for each document. This is the model's outputted JSON.\n",
        "                        The first level keys are document names, and the second level keys are entity names with values being the extracted entity.\n",
        "\n",
        "    Returns:\n",
        "    - logs (list): A list of dictionaries, each representing the log for a specific document.\n",
        "                   It includes information on whether each entity was found in the document text, if not found, it gives the model's extraction for analysis.\n",
        "\n",
        "    Workflow:\n",
        "    1. Iterates through the `entity_json` dictionary to retrieve each document's extracted entities.\n",
        "    2. For each entity, checks if it exists in the corresponding document's OCR text (case-insensitive).\n",
        "    3. Logs the results, including whether the entity was found, and stores any missing entities for further analysis.\n",
        "    4. Appends the log for each document to a master log.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  logs = [] # master log\n",
        "\n",
        "  #Checker Loop\n",
        "  for k, v in entity_json.items(): # for each key(doc name) and value (entity dict) in input json\n",
        "\n",
        "        #Internal Logging\n",
        "        json_log = {} # creating each document's log\n",
        "        json_log['Document'] = k #logging the document name\n",
        "\n",
        "        #Finding Corresponding OCR\n",
        "        target_text = ocr_json[k] #getting target text\n",
        "        #json_log['Text'] = target_text #storing it\n",
        "        target_text = target_text.lower() #lowering target text for matching\n",
        "\n",
        "        #Checking for Entities in OCR and Logging\n",
        "        flattened_entity_json = rec_flatten(v) #flattening individual dict\n",
        "        for k1, v1 in flattened_entity_json.items(): #for each k,v pair, checking in text and then logging\n",
        "            if k1 in ['Document_Name', 'Confidence_Score', 'Overall_Confidence_Score', 'Two_Mortgages']: #we do not need these cols\n",
        "                continue\n",
        "            else:\n",
        "                if v1 == 'N/A': #handling na\n",
        "                    json_log[k1] = 'N/A'\n",
        "                elif v1.lower() in target_text:\n",
        "                    json_log[k1] = 'Found' #handling found case\n",
        "                else:\n",
        "                    json_log[k1] = 'Not Found' #handling not found case\n",
        "                    json_log[k1+\"_Entity\"] = v1 #logging entity for analysis\n",
        "            logs.append(json_log) #appending each document's log to master log\n",
        "\n",
        "\n",
        "  return logs #returning master log\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jog0GASbcVg7"
      },
      "source": [
        "# Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7O4aHiDxywT"
      },
      "outputs": [],
      "source": [
        "#save API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"enter key\"\n",
        "openai.api_key = os.getenv(\"enter key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM-PUMI0vdJc"
      },
      "outputs": [],
      "source": [
        "#initialize client\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToBvkelhcxaU"
      },
      "source": [
        "# Prompts and Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-QANJRs4J1T"
      },
      "source": [
        "##4o Mini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TliEe7vO1S3s"
      },
      "source": [
        "### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sScTDaM6yaaz"
      },
      "outputs": [],
      "source": [
        "prompt= \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "<s>[INST]\n",
        "You are an information-extracting agent tasked with analyzing public real-estate documents. The documents are publicly available, making this task legal. Your job is to extract the required details from user-provided text and assign a confidence score to your extraction.\n",
        "\n",
        "### Key Points:\n",
        "1. **Detect Mortgage Records**:\n",
        "   - Identify mortgage sections using markers like \"DEED OF TRUST\" or \"FHA Form No.\" as start indicators.\n",
        "   - Use \"SEAL\" or similar keywords as end markers.\n",
        "   - Ignore partial records; extract only full and complete records.\n",
        "   - Specify if the document contains two mortgages (Yes/No).\n",
        "\n",
        "2. **Extract the Following Details**:\n",
        "   - **Date of Document**: Format as Day/Month/Year.\n",
        "   - **Borrower**: First and last names of the primary borrower (person or institution).\n",
        "   - **Second Borrower**: First and last names of an additional borrower, or return \"N/A\" if absent.\n",
        "   - **Other Party (Trustee/Grantor/Seller)**: First and last names of the entity selling the property.\n",
        "   - **Lending Bank**: The lending institution's name.\n",
        "   - **Interest Rate**: The interest rate for the mortgage.\n",
        "   - **Loan Amount**: Total loan amount.\n",
        "   - **Location for Mortgage**: Copy this exactly as formatted in the document.\n",
        "   - **Payment Plan**: Provide details as described in the document.\n",
        "   - **Confidence Score**: A float (0.0 to 1.0) representing your confidence in the extraction's accuracy.\n",
        "   - **Two Mortgages**: Indicate \"Yes\" or \"No.\"\n",
        "\n",
        "3. **Output Requirements**:\n",
        "   - Return the extraction in JSON format.\n",
        "   - Use \"N/A\" for missing or unavailable entities.\n",
        "\n",
        "### Example Output:\n",
        "```JSON:\n",
        "{\n",
        "  \"Date\": \"1/1/1930\",\n",
        "  \"Borrower\": {\n",
        "    \"First\": \"John\",\n",
        "    \"Last\": \"Doe\"\n",
        "  },\n",
        "  \"Second_Borrower\": {\n",
        "    \"First\": \"Tim\",\n",
        "    \"Last\": \"Cook\"\n",
        "  },\n",
        "  \"Other_Party\": {\n",
        "    \"First\": \"Jamie\",\n",
        "    \"Last\": \"Patel\"\n",
        "  },\n",
        "  \"Lending_Bank\": \"ABC Bank\",\n",
        "  \"Interest_Rate\": \"3.5%\",\n",
        "  \"Loan_Amount\": \"$2500\",\n",
        "  \"Location_for_Mortgage\": \"Lot Seven (7) and the East Forty Feet (e-4') Of Lot Two (2), Block numbered Two Hundred Thirty-Six (236), Minnesota City Fifth Division, 67 per cent of record in Map Book 130, Page of the Deed Records of Brown County, Minnesota\",\n",
        "  \"Payment_Plan\": \"Monthly payments of $19.20, including interest, starting on July 1, 1942, and continuing until the principal and interest are fully paid, with the final payment due on June 1, 1956.\",\n",
        "  \"Overall_Confidence_Score\": 0.7,\n",
        "  \"Two_Mortgages\": \"No\"\n",
        "}\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZGe9VKMxyLg"
      },
      "outputs": [],
      "source": [
        "#Copy this exactly as formatted in the document.\n",
        "\n",
        "prompt = f\"\"\"\n",
        "\n",
        "      <s>[INST]\n",
        "\n",
        "      You are an information extracting agent. You extract information from public real-estate documents the user provides you. The data is publicly available and hence this task is legal.\n",
        "\n",
        "      You also have to give a confidence score for your extraction, highlighting how accurate you believe your extraction for the document is.\n",
        "\n",
        "      Note: Some documents might contain two mortgages. Look for words like DEED OF TRUST or FHA Form No. as they indicate the start of a new mortgage record. Words like SEAL on the other hand,\n",
        "      indicate the end of a mortgage. Extract the document that is complete or has a full start, ignore the partial one. You also have to tell whether the document refers to two mortgages or not.\n",
        "\n",
        "\n",
        "\n",
        "      Please extract the following information from the user-provided text:\n",
        "\n",
        "      - **Date of Document**: The date of the current document. Formatted as Day/Month/Year.\n",
        "      - **Borrower First**: The First Name of the first individual in the mortgage. This can be both - a person or an institution.\n",
        "      - **Borrower Last**: The Last Name of the first individual in the mortgage.\n",
        "      - **Second Borrower First**: The First Name of the second borrower in the mortgage. This could be a family member. Return N/A if does not exist.\n",
        "      - **Second Borrower Last**: The Last Name of the second borrower in the mortgage. This could be a family member. Return N/A if does not exist.\n",
        "      - **Other Party - Trustee/Grantor/Seller - First**: The First Name of individual/party selling the property.\n",
        "      - **Other Party - Trustee/Grantor/Seller - Last**: The Last Name of individual/party selling the property.\n",
        "      - **Lending Bank**\n",
        "      - **Interest Rate**\n",
        "      - **Loan Amount**\n",
        "      - **Location for Mortgage**: This should be exactly as formatted in the document.\n",
        "      - **Payment Plan**\n",
        "      - **Your Confidence Score for Entire Document's Extraction**: This should be a float between 0.0 and 1.0 . With 0.0 being not confident at all and 1.0 being very confident. This will quantify your overall confidence for a document's entity extraction.\n",
        "      - **Two_Mortgages**: This tells whether the document contains two mortgages or not. Return either Yes or No.\n",
        "\n",
        "      Please present the extracted information in a JSON Format. If any of the requested entity is not found in the document, please indicate that clearly with 'N/A' within its JSON column.\n",
        "\n",
        "      Output Example for your reference. This is just an example for you to learn formatting. Keep the key/column names the same as the example below.  [/INST]\n",
        "\n",
        "      JSON:\n",
        "\n",
        "      {{\n",
        "        \"Date\": \"1/1/1930\",\n",
        "        \"Borrower\": {{\n",
        "            \"First\": \"John\",\n",
        "            \"Last\": \"Doe\"\n",
        "        }},\n",
        "        \"Second_Borrower\": {{\n",
        "            \"First\": \"Tim\",\n",
        "            \"Last\": \"Cook\"\n",
        "        }},\n",
        "        \"Other_Party\": {{\n",
        "            \"First\": \"Jamie\",\n",
        "            \"Last\": \"Patel\"\n",
        "        }},\n",
        "        \"Lending_Bank\": \"ABC Bank\",\n",
        "        \"Interest_Rate\": \"3.5%\",\n",
        "        \"Loan_Amount\": \"$2500\",\n",
        "        \"Location_for_Mortgage\": \"Lot Seven (7) and the East Forty Feet (e-4') Of Lot Two (2), Block numbered Two Hundred Thirty-Six (236), Minnesota City Fifth Division, 67 per cent of record in Map Book 130, Page of the Deed Records of Brown County, Minnesota\",\n",
        "        \"Payment_Plan\": \"Monthly payments of $19.20, including interest, starting on July 1, 1942, and continuing until the principal and interest are fully paid, with the final payment due on June 1, 1956.\",\n",
        "        \"Overall_Confidence_Score\": \"0.7\",\n",
        "        \"Two_Mortgages\": \"No\"\n",
        " }}\n",
        "\n",
        "       Please return only the JSON object. </s>\n",
        "\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2tIUda11VKk"
      },
      "source": [
        "### Running Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBZK62JEUY4g"
      },
      "outputs": [],
      "source": [
        "#this function will pass the prompt and ocr to the LLM. one call processes one document.\n",
        "def extract_4o_mini(text, prompt):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "    This function processes a single mortgage document using the GPT-4o-mini model.\n",
        "    It sends a given OCR input and a prompt to the model and returns the extracted\n",
        "    information in JSON format, if valid.\n",
        "\n",
        "    For the model, we reinitialize the message history for every document. Hence, the model does not have\n",
        "    access to previous documents while processing. Each input is new and is its own context. We do not\n",
        "    maintain context history to prevent hallucination or leaks.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The OCR to be processed.\n",
        "        prompt (str): The system prompt defining the task and expected output format.\n",
        "\n",
        "    Workflow:\n",
        "    1. Send the prompt and text to the GPT-4o-mini model for processing.\n",
        "    2. Extract the content of the model's response.\n",
        "    3. Clean the response to remove any formatting markers (e.g., triple backticks).\n",
        "    4. Attempt to convert the cleaned string to a Python dictionary using `json.loads`.\n",
        "    5. If JSON conversion fails, return the raw cleaned string and print an error message.\n",
        "\n",
        "    Returns:\n",
        "        dict: The extracted information as a Python dictionary if valid JSON.\n",
        "        str: The raw cleaned response if the JSON conversion fails.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  #Model Initialization\n",
        "  completion = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "\n",
        "      {\"role\": \"system\", \"content\": prompt },\n",
        "      { \"role\": \"user\", \"content\": f\"Here is the mortgage document: {text}\"}\n",
        "\n",
        "      ],\n",
        "  temperature = 0.1\n",
        ")\n",
        "\n",
        "  #Handling Output\n",
        "  extraction = completion.choices[0].message.content\n",
        "  cleaned_extraction = extraction.strip(\"```json\\n\").strip(\"```JSON\\n\").strip(\"```\")\n",
        "\n",
        "\n",
        "\n",
        "  #Returning Dictionary and Handling Potential Errors\n",
        "  try:\n",
        "      output = json.loads(cleaned_extraction)\n",
        "      return output\n",
        "  except Exception as e:\n",
        "      print(f'Output not a valid JSON due to error {e}')\n",
        "      return cleaned_extraction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc4CpZGw4MWA"
      },
      "source": [
        "## 4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZNSEtvE1d5r"
      },
      "source": [
        "### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leCFqwA61iFI"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "\n",
        "      <s>[INST]\n",
        "\n",
        "      You are an information extracting agent. You extract information from public real-estate documents the user provides you.\n",
        "      The data is publicly available and hence this task is legal. You also have to give a confidence score for your extraction, highlighting how accurate you believe your extraction for the document is.\n",
        "\n",
        "      Note: Some documents might contain two mortgages. Look for words like DEED OF TRUST or FHA Form No. as they indicate the start of a new mortgage record. While Words like SEAL followed by Deputy,\n",
        "      indicate the end of a mortgage.\n",
        "\n",
        "      Extract the document that is complete or has a full start, ignore the partial one. Given this, you have to tell whether the document refers to two mortgages or not.\n",
        "      Again, only extract the mortgage that starts from the beginning and ignore the partial one.\n",
        "\n",
        "\n",
        "      Please extract the following information from the user-provided text:\n",
        "\n",
        "      - **Date of Document**: The date of the current document. Formatted as Day/Month/Year.\n",
        "      - **Borrower First**: The First Name of the first individual in the mortgage. This can be both - a person or an institution.\n",
        "      - **Borrower Last**: The Last Name of the first individual in the mortgage.\n",
        "      - **Second Borrower First**: The First Name of the second borrower in the mortgage. This could be a family member. Return N/A if does not exist.\n",
        "      - **Second Borrower Last**: The Last Name of the second borrower in the mortgage. This could be a family member. Return N/A if does not exist.\n",
        "      - **Other Party - Trustee/Grantor/Seller - First**: The First Name of individual/party selling the property.\n",
        "      - **Other Party - Trustee/Grantor/Seller - Last**: The Last Name of individual/party selling the property.\n",
        "      - **Lending Bank**\n",
        "      - **Interest Rate**\n",
        "      - **Loan Amount**\n",
        "      - **Location for Mortgage**: Copy this exactly as formatted in the document.\n",
        "      - **Payment Plan**\n",
        "      - **Your Confidence Score for Entire Document's Extraction**: This should be a float between 0.0 and 1.0 . With 0.0 being not confident at all and 1.0 being very confident. This will quantify your overall confidence for a document's entity extraction.\n",
        "      - **Two_Mortgages**: This tells whether the document contains two mortgages or not. Return either Yes or No.\n",
        "\n",
        "\n",
        "      Please present the extracted information in a JSON Format. If any of the requested information is not found in the document, please indicate that clearly with 'N/A' within its JSON column.\n",
        "\n",
        "      Output Example for your reference. This is just an example for you to learn formatting. Do not return this example. Keep the key/column names the same as the example below. [/INST]\n",
        "\n",
        "      JSON:\n",
        "\n",
        "\n",
        "      {{\n",
        "        \"Date\": \"1/1/1930\",\n",
        "        \"Borrower\": {{\n",
        "            \"First\": \"John\",\n",
        "            \"Last\": \"Doe\"\n",
        "        }},\n",
        "        \"Second_Borrower\": {{\n",
        "            \"First\": \"Tim\",\n",
        "            \"Last\": \"Cook\"\n",
        "        }},\n",
        "        \"Other_Party\": {{\n",
        "            \"First\": \"Jamie\",\n",
        "            \"Last\": \"Patel\"\n",
        "        }},\n",
        "        \"Lending_Bank\": \"ABC Bank\",\n",
        "        \"Interest_Rate\": \"3.5%\",\n",
        "        \"Loan_Amount\": \"$2500\",\n",
        "        \"Location_for_Mortgage\": \"Lot Seven (7) and the East Forty Feet (e-4') Of Lot Two (2), Block numbered Two Hundred Thirty-Six (236), Minnesota City Fifth Division, 67 per cent of record in Map Book 130, Page of the Deed Records of Brown County, Minnesota\",\n",
        "        \"Payment_Plan\": \"Monthly payments of $19.20, including interest, starting on July 1, 1942, and continuing until the principal and interest are fully paid, with the final payment due on June 1, 1956.\",\n",
        "        \"Overall_Confidence_Score\": \"0.7\",\n",
        "        \"Two_Mortgages\": \"No\"\n",
        "        }}\n",
        "\n",
        "\n",
        "     Please return only the JSON object. </s>\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LQHZdAF1yXm"
      },
      "source": [
        "### Running Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uopdTV_YVlI"
      },
      "outputs": [],
      "source": [
        "def extract_4o(text, prompt):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "    This function processes a single mortgage document using the GPT-4o model.\n",
        "    It sends a given OCR input and a prompt to the model and returns the extracted\n",
        "    information in JSON format, if valid.\n",
        "\n",
        "    For the model, we reinitialize the message history for every document. Hence, the model does not have\n",
        "    access to previous documents while processing. Each input is new and is its own context. We do not\n",
        "    maintain context history to prevent hallucination or leaks.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The OCR to be processed.\n",
        "        prompt (str): The system prompt defining the task and expected output format.\n",
        "\n",
        "    Workflow:\n",
        "    1. Send the prompt and text to the GPT-4o-mini model for processing.\n",
        "    2. Extract the content of the model's response.\n",
        "    3. Clean the response to remove any formatting markers (e.g., triple backticks).\n",
        "    4. Attempt to convert the cleaned string to a Python dictionary using `json.loads`.\n",
        "    5. If JSON conversion fails, return the raw cleaned string and print an error message.\n",
        "\n",
        "    Returns:\n",
        "        dict: The extracted information as a Python dictionary if valid JSON.\n",
        "        str: The raw cleaned response if the JSON conversion fails.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  #Model Initialization\n",
        "  completion = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "\n",
        "      {\"role\": \"system\", \"content\": prompt },\n",
        "\n",
        "      { \"role\": \"user\", \"content\": f\"Here is the mortgage document: {text}\"}],\n",
        "\n",
        "  temperature = 0.1\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "  #Processing Output\n",
        "  extracted_data = completion.choices[0].message.content\n",
        "\n",
        "  cleaned_extraction = extracted_data.strip(\"```JSON\\n\").strip(\"```json\\n\").strip(\"```\")\n",
        "\n",
        "\n",
        "  #Converting to Dictionary and Handling Errors\n",
        "  try:\n",
        "      output = json.loads(cleaned_extraction)\n",
        "      return output\n",
        "  except Exception as e:\n",
        "      print('Output not a valid JSON due to error {e}')\n",
        "      return(cleaned_extraction )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYuXrr8FfjnJ"
      },
      "source": [
        "# Extract Entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_j6NTzqvdJc"
      },
      "outputs": [],
      "source": [
        "#This function runs all the OCR inputs through the model and loads all extractions into a single master JSON.\n",
        "\n",
        "\n",
        "def get_output_json(document_names,texts, model):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "    Runs all the OCR inputs through a chosen OpenAI and loads all extractions into a single master JSON. Handles invalid JSONs and returns them too.\n",
        "\n",
        "    Parameters:\n",
        "    - document_names (list): A list of document names corresponding to the OCR texts.\n",
        "    - texts (list): A list of OCR texts, one for each document.\n",
        "    - model (function): The model (LLM) to be used for extraction. Pass the entire extraction function.\n",
        "\n",
        "    Returns:\n",
        "    - output_json (dict): A dictionary containing the output for all documents. The keys are document names,\n",
        "                           and the values are the extracted entities in JSON format. This is a nested JSON with 2 levels.\n",
        "\n",
        "                              Level 1: Document Names as Keys, Nested JSON as Value.\n",
        "                              Level 2: Entity Names as Keys, Extractions as Value.\n",
        "\n",
        "                              This will be the final JSON Structure:\n",
        "\n",
        "                              {\n",
        "                                \"doc_1.tif\": {\n",
        "                                  \"First_Borrower\": {\n",
        "                                    \"first_name\": \"John\",\n",
        "                                    \"last_name\": \"Doe\"\n",
        "                                  },\n",
        "                                  \"Second_Borrower\": {\n",
        "                                    \"first_name\": \"Jane\",\n",
        "                                    \"last_name\": \"Smith\"\n",
        "                                  },\n",
        "                                  \"interest_rate\": \"3.5%\",\n",
        "                                  \"location_for_mortgage\": \"New York\"\n",
        "                                },\n",
        "                                \"doc_2.tif\": {\n",
        "                                  \"First_Borrower\": {\n",
        "                                    \"first_name\": \"Alice\",\n",
        "                                    \"last_name\": \"Johnson\"\n",
        "                                  },\n",
        "                                  \"Second_Borrower\": {\n",
        "                                    \"first_name\": \"Bob\",\n",
        "                                    \"last_name\": \"Brown\"\n",
        "                                  },\n",
        "                                  \"interest_rate\": \"4.0%\",\n",
        "                                  \"location_for_mortgage\": \"California\"\n",
        "                                }\n",
        "                              }\n",
        "\n",
        "    - jsons (list): A list of the JSON data returned by the model for each document.\n",
        "    - failed_jsons (list): A list of documents that failed to generate valid JSON. Each item is a dictionary\n",
        "                           containing the document name and its extracted text. To be used for debugging.\n",
        "\n",
        "    Workflow:\n",
        "    1. Iterates over the OCR texts and document names, processing each with the provided model.\n",
        "    2. For each processed document, tries to validate and store the extracted JSON.\n",
        "    3. If the JSON is valid, it adds the document's name to the extracted data and appends it to the master JSON.\n",
        "    4. If the JSON is invalid, it adds the document's name and the extracted text to the `failed_jsons` list.\n",
        "    5. Logs the progress and the runtime of the operation.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  #Storage\n",
        "  jsons = [] #raw model output\n",
        "  failed_jsons = [] #fallback for debugging\n",
        "  output_json = {}  # final output JSON for a batch\n",
        "\n",
        "\n",
        "  counter = 0\n",
        "\n",
        "  #Running the LLM\n",
        "  for text, name in tqdm(zip(texts, document_names), total = len(document_names), desc = 'Document'):\n",
        "        json_data = model(text, prompt)  # Run the model\n",
        "        jsons.append(json_data) #Append output\n",
        "\n",
        "        #Handling Valid JSONs\n",
        "        try:\n",
        "            json.loads(json.dumps(json_data))  #Try to load and dump to check JSON validity\n",
        "            #Append to master output after validation\n",
        "            json_data['Document_Name'] = name #Add name feature to each document's JSON\n",
        "            output_json[name] = json_data  # Add document to the master JSON with key as document_name\n",
        "\n",
        "            counter+=1\n",
        "            print(f'         --------Doc {counter}')\n",
        "\n",
        "        #Handling Invalid JSONs\n",
        "        except (TypeError, ValueError):\n",
        "            failed_jsons.append({'Document_Name': name, 'Extracted_Text': json_data})\n",
        "\n",
        "            print(f'Doc {counter} Failed')\n",
        "            print(f\"Skipping document '{name}' as it is not valid JSON format.\")\n",
        "\n",
        "  end_time = time.time()\n",
        "  print(f'Model run time for {len(document_names)} documents: {end_time - start_time} seconds.')\n",
        "\n",
        "  return output_json, jsons, failed_jsons\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSG0EXwf3_lF"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DYfkh9f4CmU",
        "outputId": "28d24c35-6019-43d3-e625-ad5fd57e4c95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Document:   4%|▍         | 1/25 [00:04<01:41,  4.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:   8%|▊         | 2/25 [00:08<01:39,  4.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  12%|█▏        | 3/25 [00:12<01:30,  4.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  16%|█▌        | 4/25 [00:15<01:17,  3.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  20%|██        | 5/25 [00:19<01:12,  3.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  24%|██▍       | 6/25 [00:22<01:07,  3.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  28%|██▊       | 7/25 [00:26<01:08,  3.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  32%|███▏      | 8/25 [00:29<00:59,  3.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  36%|███▌      | 9/25 [00:37<01:19,  4.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  40%|████      | 10/25 [00:53<02:05,  8.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  44%|████▍     | 11/25 [01:09<02:29, 10.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  48%|████▊     | 12/25 [01:14<01:54,  8.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  52%|█████▏    | 13/25 [01:19<01:32,  7.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  56%|█████▌    | 14/25 [01:26<01:21,  7.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  60%|██████    | 15/25 [01:43<01:45, 10.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  64%|██████▍   | 16/25 [01:57<01:43, 11.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  68%|██████▊   | 17/25 [02:02<01:15,  9.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  72%|███████▏  | 18/25 [02:08<00:59,  8.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  76%|███████▌  | 19/25 [02:13<00:44,  7.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  80%|████████  | 20/25 [02:22<00:39,  7.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  84%|████████▍ | 21/25 [02:32<00:34,  8.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  88%|████████▊ | 22/25 [02:37<00:21,  7.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  92%|█████████▏| 23/25 [02:42<00:13,  6.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rDocument:  96%|█████████▌| 24/25 [02:52<00:07,  7.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Document: 100%|██████████| 25/25 [02:58<00:00,  7.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         --------Doc 25\n",
            "Model run time for 25 documents: 178.70416808128357 seconds.\n",
            "All documents have the same keys.\n",
            "Baseline keys: {'Loan_Amount', 'Two_Mortgages', 'Second_Borrower_Last', 'Overall_Confidence_Score', 'Document_Name', 'Interest_Rate', 'Other_Party_First', 'Second_Borrower_First', 'Lending_Bank', 'Location_for_Mortgage', 'Date', 'Payment_Plan', 'Other_Party_Last', 'Borrower_First', 'Borrower_Last'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#1. Load OCR JSON\n",
        "names, texts = load_ocr_json(\"/content/output_json__two_25_2.json\") #call helper function to get the document names and texts\n",
        "\n",
        "\n",
        "#2. Extract Entities by Running LLM\n",
        "output_json, jsons, failed_jsons = get_output_json(names, texts, extract_4o) #passes all OCR JSONS sequentially through the model and collects entity JSONS\n",
        "\n",
        "\n",
        "#3. Verify Column Names and JSON Structure\n",
        "check_keys(output_json)\n",
        "\n",
        "\n",
        "#4. Save Output JSON Locally - Post Verification\n",
        "with open('output_json.json', 'w') as f:\n",
        "    json.dump(output_json, f, indent = 4)\n",
        "\n",
        "\n",
        "#5. Flatten Output JSON\n",
        "flattened_output = flatten_json(output_json)\n",
        "\n",
        "\n",
        "#6. Convert to DataFrame Object\n",
        "df = pd.DataFrame(flattened_output)\n",
        "\n",
        "\n",
        "#7. Save as CSV\n",
        "df.to_csv('output_csv.csv', index = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o_31lAFEyDm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fq41Soqbbm2k",
        "NT_FXx8IhRPm",
        "VPl1r8OpVLI6",
        "Jog0GASbcVg7",
        "c-QANJRs4J1T",
        "uc4CpZGw4MWA",
        "oYuXrr8FfjnJ",
        "DVfc1z9IhXQU",
        "X8xfUxuhcHQj",
        "PD_7DTsNjotM",
        "UmysfuIsjaJN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3 (main, Apr 19 2023, 18:51:09) [Clang 14.0.6 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cd51ebb4cb3b40addb06731ef96b4dc3454ac60ff2893a87c577360d8508737e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
